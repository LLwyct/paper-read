# An Abstraction-Based Framework for Neural Network Verification

 <!-- <font size=4> -->

- [An Abstraction-Based Framework for Neural Network Verification](#an-abstraction-based-framework-for-neural-network-verification)
  - [1 Introduction](#1-introduction)
  - [2 Background](#2-background)
    - [2.1 Neural Network](#21-neural-network)
    - [2.2 Neural Network Verification](#22-neural-network-verification)
  - [3 Network Abstraction and Refinement](#3-network-abstraction-and-refinement)
    - [3.1 Abstraction](#31-abstraction)
    - [3.2 Refinement](#32-refinement)
  - [4 A CEGAR-Based Approach](#4-a-cegar-based-approach)
    - [4.1 Generating an Initial Abstraction](#41-generating-an-initial-abstraction)
    - [4.2 Performing the Refinement Step](#42-performing-the-refinement-step)
  - [5 Implementation and Evaluation](#5-implementation-and-evaluation)
  - [6 Related Work](#6-related-work)
  - [7 Conclusion](#7-conclusion)

**摘要**

深度神经网络越来越多地用作安全关键型系统的控制器。由于神经网络是不透明的，因此验证其正确性是一项重大挑战。为了解决这个问题，最近提出了几种方法来对其进行正式验证。但是，网络规模通常是此类方法的瓶颈，很难将其应用于大型网络。在本文中，我们提出了一个框架，该框架可以通过使用过度近似来减小网络的大小，从而增强神经网络验证技术，从而使其更易于验证。

我们进行近似计算，以便如果该属性适用于较小的（抽象）网络，那么它也适用于原始网络。过度近似可能太粗糙，在这种情况下，基础验证工具可能会返回虚假的反例。在这种情况下，我们进行反例指导的优化以调整近似值，然后重复该过程。

我们的方法与许多现有的验证技术正交，并且可以与之集成。为了进行评估，我们将其与最近提出的Marabou框架进行了整合，并观察到Marabou的性能有了显着改善。我们的实验证明了我们的方法在验证更大的神经网络方面的巨大潜力。

## 1 Introduction

机器编程（MP）——软件的自动生成，它正表现出了从根本上改变软件开发方式的早期迹象。 MP使用的关键要素是深层神经网络（DNN），它已成为半自治地实现许多复杂软件系统的有效手段。

DNN是机器学习衍生出的工具：用户提供了系统应如何运行的示例，并且机器学习算法将这些示例输入DNN，且DNN能够正确处理以前从未见过的输入。

具有DNN组件的系统在图像识别，游戏，自然语言处理，计算机网络等许多领域已获得了空前的成果，通常超过了精心人工编写的类似的系统所获得的结果。 显然，这种趋势将急剧增加，DNN组件将部署在各种安全关键系统中。

DNN吸引人之处在于，（在某些情况下）它们比人工编写的软件更易于创建，同时仍能达到出色的效果。 但是，在证明方面，它们的使用也带来了挑战。 在许多最新的DNN中都观察到了不良行为。 例如，在许多情况下，对正确处理的输入的轻微扰动可能会导致严重错误。 由于提高人工代码可靠性的许多实践尚未成功应用于DNN（例如，代码审查，编码指南等），因此，如何克服DNN的不透明性尚不清楚，这可能会限制我们的能力——在部署它们之前对其进行证明。

为了减轻这种情况，形式方法社区已开始开发用于DNN形式验证的技术。 这些技术可以自动证明DNN始终满足规定的属性。 不幸的是，DNN验证问题在计算上很困难（例如，NP完备问题，甚至于简单的的规范和网络），并且随着网络规模的增加，其难度将成倍增加。 因此，尽管最近在DNN验证技术方面取得了进步，但是网络规模仍然是一个严重的限制因素。

在这项工作中，我们提出了一种技术，它可以显着提高许多现有验证技术的可伸缩性。这个想法应用了一个成熟的概念——抽象和完善[4]：用一个更小的抽象网络$N^-$替换要验证的网络$N$，然后验证$N^-$。由于$N-$较小，因此可以更有效地进行验证。并且N-将以这样的方式构造：如果$N^-$满足规范，则原始网络$N$也将满足。如果$N^-$未指定规格，则验证过程将提供反例$x$。这个$x$可能是一个真实的反例，表明原始网络$N$违反了规范，或者它是虚假的。如果$x$是虚假的，网络$N^-$将被细化以使$x$更准确（并且稍稍扩大），然后重复该过程。此方法的一个特别有用的变体是使用虚假x来指导优化过程，以使优化步骤排除反例$x$。这种称为反例指导抽象改进（CEGAR）的变体已成功应用于许多验证环境中。

作为我们技术的一部分，我们提出了一种抽象和完善神经网络的方法。我们的基本抽象步骤将两个神经元合并为一个，因此将神经元的总数减少了一个。这个基本步骤可以重复很多次，从而大大减小了网络规模。相反，通过将先前合并的神经元一分为二来进行细化，从而增加网络大小，但使其与原始神经元更加相似。关键点在于，并非所有神经元对都可以合并，因为这么做的结果可能导致网络较小，但并不是我们所期望的原始网络的过度近似。为了解决这个问题，我们首先将原始网络转换为等效网络，其中每个节点都属于四个类别之一，这取决于其边的权重及其对网络输出的影响；然后可以安全地合并来自同一类的神经元。哪些神经元合并或分裂的实际选择是通过试探法进行的。我们提出并讨论了几种可能的启发式方法。

出于评估目的，我们将我们的方法用Python框架实现并且它集成了Marabou验证工具[16]。 然后，我们使用我们的框架来验证机载防撞系统（ACAS Xu）的一组基准属性[14]。 我们的结果有力地证明了抽象在增强现有验证方案方面的潜在有用性：具体而言，在大多数情况下，增强抽象的Marabou明显优于原始的。 此外，在大多数情况下，通过验证其较小的抽象版本，确实可以展示原始DNN中的属性是否保留。

总而言之，我们的贡献是：

1. 我们提出了一个过度逼近和完善DNN的通用框架；
2. 我们提出了几种抽象和完善的启发式方法，将在我们的总体框架内使用；
3. 提供与Marabou验证工具集成并用于评估的技术实现。

本文的其余部分安排如下。 在第2节中，我们提供了有关神经网络及其验证的简要背景。 在第3节中，我们描述了用于抽象优化的DNN的通用框架。 在第4节中，我们讨论如何将这些抽象和优化步骤作为CEGAR程序的一部分来应用，然后在第5节中进行评估。在第6节中，我们讨论相关工作，并在第7节中总结。

## 2 Background

### 2.1 Neural Network

神经网络由一个输入层，一个输出层和一个或多个称为隐藏层的中间层组成。 每层都是节点的集合，称为神经元。 每个神经元通过一个或多个有向边缘连接到其他神经元。 在前馈神经网络中，第一层中的神经元接收设置其初始值的输入数据。 其余的神经元使用它们从上一层通过边缘连接到的神经元的加权值来计算其值（见图1）。 输出层提供给定输入的DNN的结果值。

DNN的类型很多，其神经元值的计算方式可能有所不同。 通常，首先根据边缘权重计算前一层神经元值的加权和，然后将激活函数应用于该加权和[8]，从而对神经元进行评估。在这里，我们将重点放在整流线性单元（ReLU）激活函数[23]上，给出为$Relu(x) = max(0, x)$。 因此，如果加权和计算得出正值，则将其保留； 否则，将其替换为零。

更形式化上的，给定一个DNN N，我们用$n$来表示DNN的层数，用$s_i$来代表第$i$层的节点数。layer 1和layer n分别代表输入层和输出层。layer 2至n-1代表隐藏层。我们用$v_{i,j}$表示第i层第j个节点的值，并且用列向量$[v_{i,1},v_{i,2},v_{i,3},...,v_{i,s_i}]^T$表示$V_i$。

通过为输入层分配$V_i$计算$V_n$来对N进行评估。这是通过连续的计算$V_i$来完成的，每次使用$V_{i-1}$来计算加权和，然后应用ReLU激活函数。具体而言，第i层（对于i>1）与大小为$s_i*s_{i-1}$的权重矩阵$W_i$和大小为$s_i$的偏置矢量$B_i$相关。如果i是一个隐藏层，则其值由$V_i = ReLU(W_iV_{i-1} + B_i)$给出，其中ReLU是逐元素应用的；输出层是由$V_n = W_nV_{n-1} + B_n$给出的（不应用ReLU）。 在不失一般性的前提下，在本文的其余部分中，我们假设所有偏差值均为0，可以忽略不计。该规则适用于直到最终计算出$V_n$。

有时我们将使用符号$w(v_{i,j},v_{i+1,k})$来表示$W_{i+1}$的条目，该条目代表第$i$层神经元$j$和第$i+1$层神经元k之间的边缘权重。 也将这个边称为$v_{i,j}$的输出边和$v_{i+1,k}$的输入边。

作为我们抽象框架的一部分，有时我们需要考虑DNN的后缀，其中忽略了DNN的第一层。 对于$1<i<n$，我们使用$N^{[i]}$表示由layers $i,i+1,...,n$组成的DNN，原始网络。其余各层的大小和权重不变，并且$N$的第$i$层被视为$N^{[i]}$的输入层。

图二展示了一个小型神经网络。网络一共有三层，分别有1、2、1个节点。他们之间的权值为$w(v_{1,1},v_{2,1})=1$，$w(v_{1,1},v_{2,2})=-1$，$w(v_{2,1},v_{3,1})=1$，$w(v_{2,2},v_{3,1})=2$。输入的值$v_{1,1} = 3$，可以推出$v_{2,1} = 3$，，因为ReLU激活函数$v_{2,2} = 0$。最后推出输出节点$v_{3,1}$的值为3。

### 2.2 Neural Network Verification

DNN验证等同于回答以下问题：给定DNN N，它把输入向量$x$映射到输出向量$y$，并给出谓词$P$和$Q$，是否存在输入$x_0$使得在$y_0=N(x_0)$时满足$P(x_0)$和$Q(y_0)$都成立？ 换句话说，验证过程确定了是否存在一个特定输入能够满足输入标准$P$，并将其映射到一个可以满足输出标准$Q$的输出。我们将$<N，P，Q>$称为**验证查询**。 与验证中一样，$Q$表示期望属性的取反。 因此，如果查询是不可满足的（UNSAT），则该属性成立；如果查询是可满足的（SAT），则$x_0$构成所讨论属性的一个反例。

不同的验证方法可能在以下方面有所不同：

1. 它们允许的神经网络的种类（特别是所使用的激活函数的种类）；
2. 输入的属性的种类；
3. 输出的属性的种类。

为简单起见，我们重点介绍采用ReLU激活功能的网络。另外，我们的输入属性将是对输入值的线性约束的合取。最后，我们假定我们的网络只有一个输出节点$y$，并且对于给定的常数$c$，输出属性为$y>c$。我们强调，这些限制是为了简单起见。通过添加编码布尔结构的神经元，可以将许多感兴趣的属性（包括具有任意布尔结构的属性）简化为上述单输出设置[14,26]。尤其是对于ACAS Xu基准系列[14]以及使用$L_∞$或$L_1$范数作为距离度量的且具有对抗性和鲁棒性的查询而言，都是如此[3,9,15]。另外，其他分段线性激活函数，例如max-pooling layers，也可以使用ReLUs进行编码[3]。

近年来，已经提出了几种技术来解决上述验证问题（第6节包括简要概述）。我们的抽象技术通过简化待验证网络，来达到与大多数技术兼容的目的，如下所述。

## 3 Network Abstraction and Refinement

因为验证神经网络的复杂性与其大小紧密相关[14]，所以我们的目标是将验证查询$\varphi1=<N,P,Q>$转换为查询$\varphi2=<\bar{N},P,Q>$，从而使抽象网络$\bar{N}$明显小于$N$（注意，性质$P$和$Q$保持不变）。我们将构造$\bar{N}$，使得它是$N$的一个过度近似，这意味着如果$\varphi2$是UNSAT，则$\varphi1$也是UNSAT。**接下来进行具体解释——为什么如果$\varphi2$是UNSAT，则$\varphi1$也是UNSAT**。更具体地说，由于DNN具有单个输出，因此我们可以将$N(x)$和$\bar{N}(x)$视为每个输入x的真值。【因为单输出的话，输出的不是一个向量而是一个常数】为了保证$\varphi2$过度近似$\varphi1$，我们将确保对于每个$x$，$N(x)≤\bar{N}(x)$；因此，$\bar{N}≤c⇒N(x)≤c$。因为我们的输出属性始终具有$N(x)>c$的形式，所以确实存在这样的情况：如果$\varphi2$是UNSAT，即对于所有x，都有$\bar{N}≤c$，那么对于所有x，$N(x)≤c$，因此$\varphi1$也是UNSAT。 我们现在提出一个框架用于生成具有此属性的各种$\bar{N}$。

### 3.1 Abstraction

我们试图定义一个抽象运算符，它通过将单个神经元与另一个神经元合并来删除网络中的单个神经元。为此，我们首先将N转换为等效网络，等效网络的神经元具有助于后续的合并。等效在这里意味着对于每个输入向量，两个网络都产生完全相同的输出。**第一，我们转换后的网络中每个隐藏神经元$v_{i,j}$将被分类为pos神经元或neg神经元。如果一个神经元所有的出边的权重均为正，则为pos神经元；如果所有权重均为负，则为负神经元。第二，与pos/neg的分类正交，每个隐藏的神经元也将被分为inc神经元或dec神经元。当我们查看$N^{[i]}$（$v_{i,j}$是其输入神经元），如果增加$v_{i,j}$的值会增加网络输出的值，则$v_{i,j}$是N的增量神经元。** 公式上，我们再解释一下上述定义，对于一个神经网络$N^{[i]}$，如果对于每两个输入向量$x1、x2$都存在，当$x1[k]=x2[k]，k!=j$且$x1[j]>x2[j]$时，都满足$N^{[i]}(x1)>N^{[i]}(x2)$，则这个神经网络的$v_{i,j}$是inc类型的节点。dec神经元是对称定义的，因此减小$x[j]$的值将增加输出。我们将首先描述这种转换（图3中显示了这种转换），接下来我们解释它如何适合我们的抽象框架。

我们第一步是将$N$转换为一个新的网络$N'$，在新网络中，每一个隐藏神经元将都被分类为pos或neg。这一步转换是通过用两个类别分别为pos和neg的神经元$v_{i,j}^+$和$v_{i,j}^-$替换每一个隐藏神经元$v_{i,j}$来完成的。对于入边，$v_{i,j}^+$和$v_{i,j}^-$都保留了原始$v_{i,j}$所有输入边的一份拷贝；但是，对于出边，$v_{i,j}^+$仅保留权重为正的输出边，而$v_{i,j}^-$仅保留权重为负的出边缘。通过将权重设置为0，可以将权重为负的输出出边从$v_{i,j}^+$中删除，对于$v_{i,j}^-$对称处理。在公式上，对于每一个神经元$v_{i-1,p}$，

$$w'(v_{i-1,p}, v_{i,j}^+)=w(v_{i-1,p}, v_{i,j})$$

$$w'(v_{i-1,p}, v_{i,j}^-)=w(v_{i-1,p}, v_{i,j})$$

其中$w'$代表新网络$N'$的权重。并且，对于每一个神经元$v_{i+1,q}$

$$
w'(v_{i,j}^+,v_{i+1,q})=
\begin{cases}
w(v_{i,j},v_{}),\quad w(v_{i,j},v_{i+1,q})>=0\\
0,\quad else
\end{cases}
$$

and

$$
w'(v_{i,j}^-,v_{i+1,q})=
\begin{cases}
w(v_{i,j},v_{}),\quad w(v_{i,j},v_{i+1,q})<=0\\
0,\quad else
\end{cases}
$$

（见图3）。对$N$的每个隐藏神经元执行一次此操作，导致网络$N'$大约是$N$的两倍。请注意，$N'$的确等于$N$，即它们的输出始终相同。

我们的第二步是将$N'$进一步更改为新网络$N''$，其中每个隐藏的神经元都将会是inc或dec（在pos或neg的基础上）。通过向后遍历$N'$的层来执行从$N'$生成$N''$的操作，每次处理单个层并可能使其神经元数量加倍：

- initial step：输出层有一个神经元y。这个神经元是一个inc节点，因为增加其值将增加网络的输出值。

- Iterative step：观察第i层，并假设第i+1层的节点已划分为inc和dec节点。观察第i层中标记为pos的神经元$v_{i,j}^+$（neg的情况是对称的）。我们将$v_{i,j}^+$替换为两个神经元$v_{i,j}^{+,I}$和$v_{i,j}^{+,D}$，分别是inc和dec。两个新的神经元都保留了$v_{i,j}^+$所有入边的副本；但是，$v_{i,j}^{+,I}$仅保留引导inc节点的出边，而$v_{i,j}^{+,D}$仅保留引导dec节点的出边。因此，对于每一个$v_{i-1,p}$和$v_{i+1,q}$，

    ![公式](https://img-blog.csdnimg.cn/20200609165405939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N3YWxsb3dibGFuaw==,size_16,color_FFFFFF,t_70)

    其中$w''$代表新网络$N''$中的权重。我们对第i层中的每个神经元执行此步骤，从而将每个神经元分类为inc或dec。

要了解这种分类的直觉，请回想一下，根据我们的假设，包括$v_{i,j}^+$在内的所有隐藏节点都使用ReLU激活函数并仅吸收非负值。由于$v_{i,j}^+$为pos，因此其所有出边均具有正权重，因此如果它的分配是增加（减少），下一层连接它的所有节点的分配也将增加（减少）。因此，我们将$v_{i,j}^+$分为两部分，并确保一个副本$v_{i,j}^{+,I}$仅连接到需要增加的节点（inc节点），而另一个副本$v_{i,j}^{+,D}$仅连接到需要减少的节点（dec节点）。这样可以确保$v_{i,j}^{+,I}$本身是inc，而$v_{i,j}^{+,D}$是dec。同样，$v_{i,j}^{+,I}$和$v_{i,j}^{+,D}$仍然是pos节点，因为它们的出边均具有正权重。

当该过程终止时，$N''$等价于$N'$，因此也等价于$N$；$N''$是$N'$的两倍，是$N'$的四倍。两个转换步骤仅对隐藏的神经元执行，而输入和输出神经元保持不变。 以下引理概括了这一点：

*Lemma 1. 通过将神经元的数量最多增加4倍，可以将任何DNN $N$转换为等效网络$N''$，其中每个隐藏的神经元为pos或neg，同时是inc或dec。*

使用引理1，我们可以不失一般性地假设输入查询$\varphi1$中的每一个DNN节点分别标记为pos/neg和inc/dec。现在我们准备构造过度近似网络$\bar{N}$。为此，我们指定了一个抽象运算符，该运算符合并了网络中的一对神经元（因此将网络大小减小了一），并且可以多次应用。唯一的限制是，要合并的两个神经元必须来自同一隐藏层，并且必须共享相同的pos/neg和inc/dec属性。【因此，将抽象应用于饱和度将导致每个隐藏层中最多包含4个神经元的网络，从而使原始网络过度近似。】当然，对于大多数合理的输入网络，这将使神经元数量的巨大减少。

抽象运算符的行为取决于要合并的神经元的属性。为简单起见，我们将重点介绍<pos,inc>案例。 令$v_{i,j}$，$v_{i,k}$为第i层的两个隐藏神经元，均分类为<pos,inc>。 由于第i层是隐藏的，因此我们知道定义了i+1和i-1层。设$v_{i-1,p}$和$v_{i+1,q}$分别表示前一层和后一层中的任意神经元。我们构造一个与$N$相同的网络$\bar{N}$，除了：（i）删除节点$v_{i,j}$和$v_{i,k}$并用新的单个节点$v_{i,t}$替换； （ii）移除所有接触节点$v_{i,j}$或$v_{i,k}$的边，而其他边未接触。最后，我们为新节点$v_{i,t}$添加新的入边和出边，如下所示：

- Incoming edges: $\bar{w}(v_{i-1,p},v_{i,t}) = max\{w(v_{i-1,p}, v_{i,j}),w(v_{i-1,p}, v_{i,k})\}$
- Outcoming edges: $\bar{w}(v_{i,t},v_{i+1,q}) = w(v_{i,j}, v_{i+1,q})+w(v_{i,k}, v_{i+1,q})$

其中，$\bar{w}$表示新网络$\bar{N}$中的权重。一个说明性的例子如图4所示。直观地讲，抽象的定义旨在确保新节点$v_{i,t}$总是比两个原始节点$v_{i,j}$和$v_{i,k}$对网络的输出贡献更多，因此新网络的每个输入都会产生比原始输出更大的输出。通过我们定义新神经元$v_{i,t}$的入边的方式，我们可以确保对于传递给$N$和$\bar{N}$的每个输入$x$，在$\bar{N}$中分配给$v_{i,t}$的值大于分配给原始网络中两个神经元的$v_{i,j}$和$v_{i,k}$的值。这对我们很有用，因为$v_{i,j}$和$v_{i,k}$均为inc——因此增加它们的值会增加输出值。根据我们对出边的定义，与N相比，第i+1层中任何inc节点的值在N中都增加，而任何dec节点的值都在减少。根据定义，这意味着网络的整体产出会增加。

<neg,inc>例子的抽象操作与上述操作相同。 对于其余两种情况，即<pos,dec>和<neg,dec>，定义中的max运算符将替换为min。

下一个引理（由于缺少空间而省略了证明）证明了使用我们的抽象步骤是合理的，并且可以在每个抽象应用中应用一次：

*引理2. 对N使用一次抽象的应用，从而由N派生出$\bar{N}$。所有x都满足$\bar{N}(x) >= N(x)$。*

> *Lemma 2. Let $\bar{N}$ be derived from N by a single application of abstract. For
every x, it holds that N¯(x) ≥ N(x).*

### 3.2 Refinement

前面提到的**抽象操作**通过合并神经元来减小网络大小，但是以准确性为代价：对于某些输入$x_0$，原始网络返回$N(x_0)=3$，则抽象创建的过度近似网络$\bar{N}$可能会返回$\bar{N}(x_0)=5$。如果我们的目标是证明永远不会出现N(x)>10这种情况，那么过度近似可能是可以满足的：我们可以证明$\bar{N}(x)<=10$永远满足，并且这已经足够了。但是，如果我们的目标是证明永远不会出现$N>4$的情况，则过度近似是不可以满足的：该属性可能满足$N$，但是由于$\bar{N}(x)=5>4$，我们的验证程序将返回$x_0$作为***虚假反例***（一个反例是$\bar{N}$的反例而不是$N$的反例）。为了处理这种情况，我们定义了一个精细运算符，**refine**，即抽象的逆：将$\bar{N}$转化为另一个过度近似，$\bar{N}'$，它的性质是，对于每一个$x$都有$N(x)<=\bar{N}'(x)<=\bar{N}(x)$。如果$\bar{N}'(x_0)=3.5$，则一个合适的过度近似可能永远不会显示$N(x)>4$。在这一节我们定义了**refine**操作符，并且我们将在第四节解释如何把 **abstract**和**refine** 用作基于CEGAR的验证方案的一部分。

回想一下，abstract将两个共享相同属性的神经元合并在一起。经过一系列abstract的应用后，所得网络的每个隐藏层i都可以视为原始网络的隐藏层i的划分，其中每个划分都包含共享相同属性的原始，具体的神经元。在抽象网络中，每个划分都由单个抽象神经元表示。根据抽象运算符的定义确定此抽象神经元的输入、输出边缘上的权重。例如，在抽象神经元$\bar{v}$表示一组都有$<pos,inc>$属性的具体神经元$\{v_1,v_2,...,v_n\}$的情况下，每个入边到$\bar{v}$的权重由下式给出：

$$\bar{w}(u,v)=max(w(u,v_1),w(u,v_2),...,w(u,v_n))$$

其中$u$代表尚未抽象的神经元，而$w$是原始网络的权重函数。这里的关键点是，合并$v_1,v_2,...,v_n$的抽象操作的顺序并不重要，反而是它们现在已经组合在一起的事实才决定抽象网络的权重。以下推论是引理2的直接结果，它建立了抽象应用程序和分区序列之间的这种联系：

*Corollary 1. 假设N为DNN，其中每个隐藏神经元分别标记为pos/neg和inc/dec，而$P$为$N$的隐藏神经元的划分，即仅将来自同一层的、共享相同神经元标签的隐藏神经元组合在一起。然后$N$和$P$产生一个抽象神经网络$\bar{N}$，它是通过执行一系列根据P的划分将神经元分组在一起的抽象操作而获得的。该$\bar{N}$是N的一个过度近似。*

在某种意义上，我们现在定义一个**refine**操作，即抽象的逆过程。refine将通过一系列abstract操作从$N$生成的DNN $\bar{N}$作为输入，并将$\bar{N}$中的神经元一分为二。形式上，操作员会收到原始网络$N$，划分$P$和通过将单个类别一分为二从$P$获得的更好的划分$P'$。 然后，操作员返回一个新的抽象网络$\bar{N}'$，它是根据$P'$对$N$的抽象。

由于推论1，并且由于refine返回的$\bar{N}$对应于N的隐藏神经元的划分$P'$，因此很容易证明$\bar{N}$确实是$N$的一个过度逼近。我们需要的其他有用的属性是以下内容：

*Lemma 3. 假设$\bar{N}$是N的抽象，并且假设$\bar{N}'$是由$\bar{N}$通过单步refine得到的。那么对于任意输入$x$都满足$\bar{N}(x)>=\bar{N}'(x)>=N(x)$*

等式的第二部分成立是因为$\bar{N}'$是$N$的过度近似（根据Corollary 1），公式的第一部分成立是因为$\bar{N}(x)$可以由$\bar{N}'(x)$通过一步abstract获得。

当然，实际上，从初始$N$开始并应用一系列abstract运算是实现refine的一种浪费的方式。取而代之的是，有可能在单个步骤中将抽象神经元一分为二，并仅检查映射到两个新抽象神经元的具体神经元，以确定新的边缘权重。

> Of course, in practice, starting from the original N and applying a sequence
of abstract operations is a wasteful way of implementing refine. Instead, it is
possible to split an abstract neuron in two in a single step and examine just the
concrete neurons that are mapped to the two new abstract neurons to determine
the new edge weights.

## 4 A CEGAR-Based Approach

在第三节中我们定义了abstract操作，它以损失网络精度为代价减少了神经网络的大小，以及定义了增加网络大小并恢复精度的逆运算符refine。这些组件与黑盒验证程序Verify一起可以分派$p=<N,P,Q>$形式的查询，现在允许我们设计用于DNN验证的abstraction-refinement算法，命名为Alg.1（我们假设输入网络中所有隐藏的神经元已被标记为pos/neg和inc/dec）。

![算法1](https://img-blog.csdnimg.cn/20200615162330538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N3YWxsb3dibGFuaw==,size_16,color_FFFFFF,t_70)

因为我们的过度近似网络$\bar{N}$是通过abstract和refine的应用获得的，并且由于我们假设基本的Verify过程是可靠的，所以引理2和3保证了Alg.1的可靠性。此外，算法总会结束：之所以如此，是因为首先执行所有abstract步骤，然后执行一系列refine步骤。因为除了步骤1之外没有执行其他abstract操作，所以在有限的许多refine步骤之后，$\bar{N}$将变得与$N$相同，此时将找不到虚假的反例，并且该算法将以SAT或UNSAT终止。当然，仅在保证基础验证过程终止时才保证算法终止。

我们有意使算法中的两个步骤变得模棱两可：第1步，计算初始过度近似，第9步，由于发现虚假的反例，对当前的抽象进行细化。动机是使Alg.1具有一般性，并通过使用不同的启发式方法来执行步骤1和9来定制Alg.1，具体使用什么方法取决于当前的问题。下面我们提出一些这样的启发式方法。

### 4.1 Generating an Initial Abstraction

生成initial abstraction的最幼稚的方法是把abstract运算符用到饱和。如前所述，abstract操作可以将给定层中共享相同属性的任何一对隐藏神经元合并在一起。由于存在四个可能的属性组合，因此这将导致网络的每个隐藏层具有四个或更少的神经元。但是，对于相当大的DNN，我们希望这种抽象非常粗糙，因此在达到SAT或UNSAT答案之前，它可能产生多轮优化。

一个不同的，用更少的refine步骤就可以产生抽象的探索如下。首先，我们选择一组有限的输入点的集合$X=\{x_1,...,x_n;\}$，全部满足输入属性$P$。这些点可以随机生成，也可以根据输入空间的某些覆盖准则生成。然后，将$X$中的点用作估算抽象何时变得过于粗糙的指标——在每个抽象步骤之后，我们检查该属性是否仍适用于$x_1,...,x_n$，如果不是，停止抽象。确切的技术出现在Alg.2中，用于执行Alg.1的步骤1。

![算法2](https://img-blog.csdnimg.cn/2020061516251365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N3YWxsb3dibGFuaw==,size_16,color_FFFFFF,t_70)

Alg.2解决的另一点，除了应该执行多少轮抽象外，是每次abstract操作中应该合并哪对神经元。这也是由启发式确定的。由于不论我们选择任何一对神经元都将导致网络规模的减小，因此我们的策略是偏向于选择能导致更精确近似的神经元来合并。误差是由abstract运算符内的max和min运算符引起的：例如，在max的情况下，每对输入边的权重为$a,b$被权重$max(a,b)$的单个边替换。我们在这里的策略是合并$|a-b|$最大值（权重为a和b的所有输入边）最小的那对神经元。凭直觉，这导致$max(a,b)$接近a和b——这反过来导致了一个比原始网络小但在权重上接近的过度近似网络。

举一个小例子，考虑图4中左侧的网络。这个网络中有三对可以使用abstract操作的神经元。考虑$v_1,v_2$：这些神经神经元中$|a-b|$的最大值是$|(1-4)|,|(-2)-(-1)| = 3$。对于节点13，最大值是1，对于节点23，最大值是2。根据算法2中的策略我们应该首先给节点13应用abstract操作。

### 4.2 Performing the Refinement Step

当发现虚假的反例$x$时，执行refinement步骤，表明抽象网络过于粗糙。换句话说，我们的抽象步骤，特别是用于为抽象神经元选择边缘权重的max和min运算符，导致抽象网络的输出对于输入$x$来说太大了，现在我们需要减少它。因此，我们的优化策略旨在以某种方式应用优化，从而显着降低抽象网络的输出。

一种启发式方法（我们称为*基于权重的优化*）是寻找当前映射到抽象神经元$\bar{v}$中的具体神经元$v$，以使$v$和$\bar{v}$的输入权重显着不同。这表明通过将$v$映射到$\bar{v}$中，我们已经执行了粗略的近似，并且将$v$从$\bar{v}$中分离出来可以恢复精度。该启发式方法在Alg.3中已正式定义。该算法仅遍历原始神经元，查找由于当前抽象而变化最大的边的权重，然后在该边的末端对神经元执行细化。

![算法3](https://img-blog.csdnimg.cn/20200615172730399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N3YWxsb3dibGFuaw==,size_16,color_FFFFFF,t_70)

例如，让我们使用Alg.3为图4的右侧网络选择一个优化步骤。假设首先考虑v1。在抽象网络中，$\bar{w}(x_1,\bar{v}_1)=4$$\bar{w}(x_2,\bar{v}_1)=-1$;而在原始网络中，$w(x_1,v_1)= 1$且$w(x_2,v_1)=-2$。因此，为$v1$计算的最大值$m$为$|w(x_1,v_1)-w(x_1,\bar{v}_1)j =3|$。m的值大于$v_2(0)$和$v_3(2)$，因此选择$v_1$作为优化步骤。执行此步骤后，$v_2$和$v_3$仍映射到单个抽象神经元，而$v_1$映射到抽象网络中的单独神经元。

基于权重的refinement试图尽可能减少输出值y，但未考虑验证过程发现的反例x。考虑一种情况，其中基于抽象神经元的输入权重选择抽象神经元$\bar{v}$，但是当在反例x上评估网络时，实际上将v赋值为0。通过执行此优化步骤，我们可能根本不会更改x的网络输出。直观地讲，通过将改进工作集中在感兴趣的输入区域中承担小任务的神经元上，我们可能会忽略其他选择，这些选择可能会大大降低网络的输出。

为了解决这种情况，我们建议更改Alg.3，使其以反例为指导。具体来说，我们建议调整Alg.3的第4行和第5行。将抽象网络中出现的项$|w(v_{i,j},v_{i-1,k})-\bar{w}(\bar{v}_{i,j},\bar{v}_{i-1,k'})|$乘以神经元$\bar{v}$的值，当网络被反例x评估时。这种启发式方法（我们称为反例指导的优化）同时考虑了边缘权重和发现的反例，并且有可能忽略了一些神经元——如果这些神经元被分配了较小的值，则基于权重的优化可能会选择的神经元。

## 5 Implementation and Evaluation

我们的abstraction-refinement框架的实现包括脚本，该脚本首先读取NNet格式的DNN[13]和要验证的属性，再如第4节所描述的那样创建一个初始的抽象DNN，最后调用黑盒验证引擎并执行refinement如第4节中所述。当基础引擎返回UNSAT或分配了一个原始网络的真实反例时，该过程终止。为了进行实验，我们将我们的框架与Marabou DNN验证引擎集成在一起[16]。

我们的实验包括验证45个ACAS Xu DNN在避免机载碰撞方面的若干属性[13,14]。这些网络中的每一个网络都有300个隐藏节点，分布在6个层中，当应用第3.1节中的转换时，会生成1200个神经元。图5描绘了使用我们的方法验证这45个网络的结果，一次是采用朴素的抽象方案，其中每个隐藏层最初具有4个神经元（x轴），一次是根据Alg.2创建初始抽象（y轴）。左图显示了终止过程之前所需的优化步骤数。它表明，确实可以在比原始网络小得多的抽象网络上证明其性能（即无需进行有效地恢复原始网络的许多优化步骤）;并且使用Alg2通常比使用朴素方案要少许多refine步骤。右图显示了Marabou在abstraction-refinement过程中解决验证查询所花费的总时间（以秒为单位的对数刻度，以秒为单位，超时时间为20小时）。有趣的是，它表明朴素的方法有时会导致更快的查询解决时间，即，尽管查询数量更多，但它们显然更易于Marabou解决。

接下来，我们比较了基于权重和反例引导的方法来执行第4.2节中讨论的优化步骤。结果显示在图6中。和以前一样，我们比较了所需refine步骤的数量（左图）和Marabou解决生成的查询所需的时间（右图）。结果表明，反例指导方法（x轴）的优越性，特别是因为它导致的超时大大减少。

最后，我们使用先前实验中发现的最佳配置，将我们的抽象增强型Marabou与原始版本进行了比较。图7中的图表将原生Marabou的总查询求解时间（y轴）与我们的方法（x轴）进行了比较。我们观察到，尽管必须验证大约四倍大的网络（由于进行预处理），但抽象增强版本的平均性能明显优于原生Marabou，通常可以更快地解决数量级的查询。这些结果清楚地表明了将我们的技术与现有的验证引擎结合在一起以提高其性能的可用性。

## 6 Related Work

近年来，已经提出了多种方案来验证神经网络。除了Marabou[16,17]，还包括Marabou的前身Reluplex[14,19]，Planet求解器[6]，BaB求解器[2]，Sherlock求解器[5]，ReluVal求解器[30]等其他求解去（例如[21,24,29,31]）。其他方法使用合理但不完善的策略，例如输入空间离散化[12]或抽象解释[7]。 我们的方法可以与任何完善的求解器集成为引擎。不完善的方法也可以使用，可能会提供更好的性能，但可能会导致无法终止。

一些现有的DNN验证技术结合了抽象元素。在[25]中，作者使用抽象将Sigmoid激活函数与矩形集合过度近似。如果他们生成的抽象验证查询是UNSAT，则原始查询也是如此。当发现虚假的反例时，执行随意优化步骤。作者报告说，可扩展性有限，只能处理具有几十个神经元的网络。抽象技术也出现在**AI2**方法中[7]，但是与DNN本身相反，输入属性和可到达区域被过度近似。

## 7 Conclusion

随着深度神经网络的普及以及即将集成到安全关键型系统中，迫切需要可扩展的技术来对其进行验证和推理。但是，这些网络的规模构成了严峻的挑战。基于抽象的技术可以通过用一个小规模版本的待验证网络代替原网络来减轻此困难，从而不会影响验证过程的可靠性。我们在这里提出的基于抽象的方法可以大大减少网络规模，从而提高现有验证技术的性能。

将来，我们计划在多个方面继续这项工作。首先，我们打算研究可以将抽象神经元分成两个任意大小的神经元的refine启发法。另外，我们将研究使用ReLU以外的其他激活函数网络的抽象方案。最后，我们计划使抽象方案可并行化，从而允许用户使用多个工作程序节点来探索抽象和细化步骤的不同组合，期望可以实现更快的收敛。
